# -*- coding: utf-8 -*-
"""Research_1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DcTJuYSC37-5rf7750dlSg8I6P5K5i_k
"""

pip install plotly

import numpy as np
import pandas as pd

from google.colab import drive
drive.mount('/content/drive')

data=pd.read_csv('/content/drive/MyDrive/research-1/dataset 1/Dataset_Heart_Disease.csv')

data.info()

data.shape

data.head()

data.drop(columns=['Unnamed: 0'], inplace=True)

"""EDA"""

data.describe().T

data.nunique()

for col in data.select_dtypes(include=[np.number]).columns:
  print(f"{col} has {data[col].nunique()} unique value")

# Function for insighting summary information about the column

def first_looking(col):
    print("column name    : ", col)
    print("--------------------------------")
    print("per_of_nulls   : ", "%", round(data[col].isnull().sum()/data.shape[0]*100, 2))
    print("num_of_nulls   : ", data[col].isnull().sum())
    print("num_of_uniques : ", data[col].nunique())
    print(data[col].value_counts(dropna = False))

first_looking("target")

print(data["target"].value_counts())
data["target"].value_counts().plot(kind="pie", autopct='%1.1f%%', figsize=(10,10));

total_cnt = data['target'].count()
labels = ['Normal','Heart Disease']
plt.figure(figsize=(12,6))
sns.set(font_scale = 2)
sns.set_style("white")
ax = sns.countplot(data=data, x='sex',hue='target',palette='Blues_r')
for p in ax.patches:
    x, height, width = p.get_x(), p.get_height(), p.get_width()
    ax.text(x + width / 2, height + 10, f'{height} / {height / total_cnt * 100:2.1f}%', va='center', ha='center', size=20)
plt.legend(labels=labels)
sns.despine()

data[data['target']==0].describe().T.style.background_gradient(subset=['mean','std','50%','count'], cmap='RdPu')

data[data['target']==1].describe().T.style.background_gradient(subset=['mean','std','50%','count'], cmap='RdPu')

numerical= data.drop(['target'], axis=1).select_dtypes('number').columns

categorical = data.select_dtypes('object').columns

print(f'Numerical Columns:  {data[numerical].columns}')
print('\n')
print(f'Categorical Columns: {data[categorical].columns}')

import matplotlib.pyplot as plt
import pandas as pd

# Assuming you have a DataFrame named 'data' and a list of numerical columns named 'numerical'
# Replace 'numerical' with the actual list of numerical columns from your DataFrame
for column in numerical:
    plt.hist(data[column], bins=10)  # Adjust bins as needed
    plt.title(f'Histogram of {column}')
    plt.xlabel('Value')
    plt.ylabel('Frequency')
    plt.show()

"""**Random Forest Classifier**"""

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import matplotlib.pyplot as plt
import seaborn as sns

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

from sklearn.preprocessing import LabelEncoder
data.head(2)

li = ['sex','chest pain type','resting ecg','exercise angina','ST slope']
for i in li:
    data[i] = LabelEncoder().fit_transform(data[i])

sns.heatmap(data.corr(),annot=True,fmt ='.2f',mask = np.triu(data.corr()))

import warnings
warnings.filterwarnings('ignore')

"""outliers"""

plt.figure(figsize=(15,15))
for i in enumerate(data.columns):
    plt.subplot(3,4,i[0]+1)
    sns.boxplot(data[i[1]])

def remove(data):
  Q1 = np.percentile(data, 25, interpolation = 'midpoint')

# Third quartile (Q3)
  Q3 = np.percentile(data, 75, interpolation = 'midpoint')
  iqr = Q3 - Q1
  percentile25 = data.quantile(0.25)
  percentile75 = data.quantile(0.75)

  upper_limit = percentile75 + 1.5 * iqr
  lower_limit = percentile25 - 1.5 * iqr
  data = np.where(
    data > upper_limit,upper_limit,
    np.where(
        data < lower_limit,
        lower_limit,
        data
    )
  )
  return data

li =['resting bps','cholesterol','max heart rate','oldpeak']
for i in li:
    data[i]=remove(data[i])

plt.figure(figsize=(15,15))
for i in enumerate(data.columns):
    plt.subplot(3,4,i[0]+1)
    sns.boxplot(data[i[1]])

plt.figure(figsize=(15,15))
for i in enumerate(data.columns):
    plt.subplot(3,4,i[0]+1)
    sns.distplot(data[i[1]])

feature = data.drop('target',axis=1)
label = data['target']

from sklearn.model_selection import train_test_split
trainF,testF,trainL,testL = train_test_split(feature,label,random_state=42,test_size=.2)
from sklearn.ensemble import RandomForestClassifier
model = RandomForestClassifier()
model.fit(trainF,trainL)
pred = model.predict(testF)
from sklearn.metrics import classification_report
print(classification_report(pred,testL))

label.value_counts()

from sklearn.preprocessing import StandardScaler

li = ['age','resting bps','cholesterol','max heart rate','oldpeak']
for i in li:
    feature[i] = StandardScaler().fit_transform(feature[[i]])

plt.figure(figsize=(15,15))
for i in enumerate(data.columns):
    plt.subplot(3,4,i[0]+1)
    sns.distplot(data[i[1]])

for i in [2,3,4,5]:
    x = data['oldpeak']**(1/i)
    sns.distplot(x)
    plt.show()

data['oldpeak'] = data['oldpeak']**(1/3)

data['target'].value_counts()

from imblearn.combine  import SMOTEENN
os = SMOTEENN(random_state = 42)
feature,label = os.fit_resample(feature,label)

from sklearn.model_selection import train_test_split
trainF,testF,trainL,testL = train_test_split(feature,label,random_state=42,test_size=.2)

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, f1_score, accuracy_score, recall_score, roc_auc_score
from sklearn.model_selection import train_test_split

# Splitting the data into training and testing sets
trainF, testF, trainL, testL = train_test_split(feature, label, random_state=42, test_size=.2)

# Initializing the Random Forest classifier
RF_model = RandomForestClassifier(class_weight="balanced", random_state=101)

# Training the model
RF_model.fit(trainF, trainL)

# Making predictions
y_pred = RF_model.predict(testF)

# Evaluating the model
print(classification_report(y_pred, testL))

# Calculating evaluation metrics
rf_f1 = f1_score(testL, y_pred)
rf_acc = accuracy_score(testL, y_pred)
rf_recall = recall_score(testL, y_pred)
rf_auc = roc_auc_score(testL, y_pred)

from sklearn.model_selection import GridSearchCV

params = {
    'n_estimators':[x**2 for x in range(3,10)],
    'n_jobs':[-1,None],
    "random_state":[42,None]
}
cv = GridSearchCV(model,params,cv=5)
cv.fit(trainF,trainL)

print(cv.best_score_)

"""Decision Tree"""

from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, f1_score, accuracy_score, recall_score, roc_auc_score

# Assuming 'feature' and 'label' are already defined
# Splitting the data into training and testing sets
trainF, testF, trainL, testL = train_test_split(feature, label, random_state=42, test_size=.2)

# Initializing the Decision Tree classifier
decision_tree_model = DecisionTreeClassifier()

# Training the model
decision_tree_model.fit(trainF, trainL)

# Making predictions
pred = decision_tree_model.predict(testF)

# Evaluating the model
print(classification_report(pred, testL))

# Calculating evaluation metrics
dt_f1 = f1_score(testL, pred)
dt_acc = accuracy_score(testL, pred)
dt_recall = recall_score(testL, pred)
dt_auc = roc_auc_score(testL, pred)

"""logistic regression"""

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, f1_score, accuracy_score, recall_score, roc_auc_score

# Assuming 'feature' and 'label' are already defined
# Splitting the data into training and testing sets
trainF, testF, trainL, testL = train_test_split(feature, label, random_state=42, test_size=.2)

# Initializing the logistic regression model
logistic_model = LogisticRegression()

# Training the model
logistic_model.fit(trainF, trainL)

# Making predictions
pred = logistic_model.predict(testF)

# Evaluating the model
print(classification_report(pred, testL))

# Calculating evaluation metrics
log_f1 = f1_score(testL, pred)
log_acc = accuracy_score(testL, pred)
log_recall = recall_score(testL, pred)
log_auc = roc_auc_score(testL, pred)

"""gradient boost"""

from sklearn.ensemble import GradientBoostingClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, f1_score, accuracy_score, recall_score, roc_auc_score

# Assuming 'feature' and 'label' are already defined
# Splitting the data into training and testing sets
trainF, testF, trainL, testL = train_test_split(feature, label, random_state=42, test_size=.2)

# Initializing the Gradient Boosting classifier
gradient_boosting_model = GradientBoostingClassifier()

# Training the model
gradient_boosting_model.fit(trainF, trainL)

# Making predictions
pred = gradient_boosting_model.predict(testF)

# Evaluating the model
print(classification_report(pred, testL))

# Calculating evaluation metrics
gb_f1 = f1_score(testL, pred)
gb_acc = accuracy_score(testL, pred)
gb_recall = recall_score(testL, pred)
gb_auc = roc_auc_score(testL, pred)

"""svm"""

from sklearn.svm import SVC
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, f1_score, accuracy_score, recall_score, roc_auc_score

# Assuming 'feature' and 'label' are already defined
# Splitting the data into training and testing sets
trainF, testF, trainL, testL = train_test_split(feature, label, random_state=42, test_size=.2)

# Initializing the SVM classifier
svm_model = SVC()

# Training the model
svm_model.fit(trainF, trainL)

# Making predictions
pred = svm_model.predict(testF)

# Evaluating the model
print(classification_report(pred, testL))

# Calculating evaluation metrics
svm_f1 = f1_score(testL, pred)
svm_acc = accuracy_score(testL, pred)
svm_recall = recall_score(testL, pred)
svm_auc = roc_auc_score(testL, pred)

"""XGboost"""

import xgboost as xgb
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, f1_score, accuracy_score, recall_score, roc_auc_score

# Assuming 'feature' and 'label' are already defined
# Splitting the data into training and testing sets
trainF, testF, trainL, testL = train_test_split(feature, label, random_state=42, test_size=.2)

# Convert data into DMatrix format
dtrain = xgb.DMatrix(trainF, label=trainL)
dtest = xgb.DMatrix(testF)

# Parameters for XGBoost
params = {
    'objective': 'binary:logistic',
    'eval_metric': 'logloss'
}

# Training the XGBoost model
num_round = 100
xgb_model = xgb.train(params, dtrain, num_round)

# Making predictions
pred_probs = xgb_model.predict(dtest)
pred = [1 if x > 0.5 else 0 for x in pred_probs]

# Evaluating the model
print(classification_report(testL, pred))

# Calculating evaluation metrics
xgb_grid_f1 = f1_score(testL, pred)
xgb_grid_acc = accuracy_score(testL, pred)
xgb_grid_recall = recall_score(testL, pred)
xgb_grid_auc = roc_auc_score(testL, pred)

"""KNN"""

from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, f1_score, accuracy_score, recall_score, roc_auc_score

# Assuming 'feature' and 'label' are already defined
# Splitting the data into training and testing sets
trainF, testF, trainL, testL = train_test_split(feature, label, random_state=42, test_size=.2)

# Initializing the KNN classifier
knn_model = KNeighborsClassifier()

# Training the model
knn_model.fit(trainF, trainL)

# Making predictions
pred = knn_model.predict(testF)

# Evaluating the model
print(classification_report(pred, testL))

# Calculating evaluation metrics
knn_f1 = f1_score(testL, pred)
knn_acc = accuracy_score(testL, pred)
knn_recall = recall_score(testL, pred)
knn_auc = roc_auc_score(testL, pred)

import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# Create DataFrame with model names and their corresponding metrics
compare = pd.DataFrame({
    "Model": ["Logistic Regression", "SVM", "KNN", "Decision Tree", "Random Forest","GradientBoost", "XGBoost"],
    "F1": [log_f1,svm_f1,knn_f1, dt_f1, rf_f1, gb_f1, xgb_grid_f1],
    "Recall": [log_recall, svm_recall, knn_recall, dt_recall, rf_recall, gb_recall, xgb_grid_recall],
    "Accuracy": [log_acc, svm_acc, knn_acc, dt_acc, rf_acc, gb_acc, xgb_grid_acc],
    "ROC_AUC": [log_auc, svm_auc, knn_auc, dt_auc, rf_auc, gb_auc,xgb_grid_auc]
})

# Define a function to label the bars in the plot
def labels(ax):
    for p in ax.patches:
        width = p.get_width()                        # get bar length
        ax.text(width,                               # set the text at 1 unit right of the bar
                p.get_y() + p.get_height() / 2,      # get Y coordinate + X coordinate / 2
                '{:1.3f}'.format(width),             # set variable to display, 2 decimals
                ha='left',                           # horizontal alignment
                va='center')                         # vertical alignment

# Create subplots for each metric comparison
plt.figure(figsize=(14, 14))

plt.subplot(411)
compare = compare.sort_values(by="F1", ascending=False)
ax = sns.barplot(x="F1", y="Model", data=compare, palette="Blues_d")
labels(ax)
plt.xlabel('F1 Score')
plt.ylabel('Model')
plt.title('Comparison of Models based on F1 Score')

plt.subplot(412)
compare = compare.sort_values(by="Recall", ascending=False)
ax = sns.barplot(x="Recall", y="Model", data=compare, palette="Blues_d")
labels(ax)
plt.xlabel('Recall')
plt.ylabel('Model')
plt.title('Comparison of Models based on Recall')

plt.subplot(413)
compare = compare.sort_values(by="Accuracy", ascending=False)
ax = sns.barplot(x="Accuracy", y="Model", data=compare, palette="Blues_d")
labels(ax)
plt.xlabel('Accuracy')
plt.ylabel('Model')
plt.title('Comparison of Models based on Accuracy')

plt.subplot(414)
compare = compare.sort_values(by="ROC_AUC", ascending=False)
ax = sns.barplot(x="ROC_AUC", y="Model", data=compare, palette="Blues_d")
labels(ax)
plt.xlabel('ROC-AUC')
plt.ylabel('Model')
plt.title('Comparison of Models based on ROC-AUC')

plt.tight_layout()
plt.show()