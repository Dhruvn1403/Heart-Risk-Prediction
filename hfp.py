# -*- coding: utf-8 -*-
"""HFP.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bpX5YKqAoec1wRT5UxvkOd_XEqK7ba0v
"""

import numpy as np
import pandas as pd

data=pd.read_csv('/content/drive/MyDrive/research-1/dataset/heart.csv')

data.info()

data.shape

data.head()

"""**Linear Regression**"""

continuous_features = ['age', 'trestbps', 'chol', 'thalach', 'oldpeak']

"""logistic regression"""

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report

# Load the data
data = pd.read_csv('/content/drive/MyDrive/research-1/dataset/heart.csv')

# Convert categorical variables to dummy/indicator variables
data = pd.get_dummies(data, columns=['Sex', 'ChestPainType', 'RestingECG', 'ExerciseAngina', 'ST_Slope'], drop_first=True)

# Separate features and target variable
X = data.drop('HeartDisease', axis=1)
y = data['HeartDisease']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create a logistic regression model
model = LogisticRegression()

# Train the model
model.fit(X_train, y_train)

# Make predictions on the test set
y_pred = model.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy:.2f}")

# Display classification report
print("Classification Report:\n", classification_report(y_test, y_pred))

"""decision tree"""

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, classification_report

# Load the data
data = pd.read_csv('/content/drive/MyDrive/research-1/dataset/heart.csv')

# Convert categorical variables to dummy/indicator variables
data = pd.get_dummies(data, columns=['Sex', 'ChestPainType', 'RestingECG', 'ExerciseAngina', 'ST_Slope'], drop_first=True)

# Separate features and target variable
X = data.drop('HeartDisease', axis=1)
y = data['HeartDisease']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create a decision tree classifier
model = DecisionTreeClassifier(random_state=42)

# Train the model
model.fit(X_train, y_train)

# Make predictions on the test set
y_pred = model.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy:.2f}")

# Display classification report
print("Classification Report:\n", classification_report(y_test, y_pred))

"""random forest 1"""

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report

# Load the data
data = pd.read_csv('/content/drive/MyDrive/research-1/dataset/heart.csv')

# Convert categorical variables to dummy/indicator variables
data = pd.get_dummies(data, columns=['Sex', 'ChestPainType', 'RestingECG', 'ExerciseAngina', 'ST_Slope'], drop_first=True)

# Separate features and target variable
X = data.drop('HeartDisease', axis=1)
y = data['HeartDisease']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create a Random Forest classifier
model = RandomForestClassifier(n_estimators=100, random_state=42)

# Train the model
model.fit(X_train, y_train)

# Make predictions on the test set
y_pred = model.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy:.2f}")

# Display classification report
print("Classification Report:\n", classification_report(y_test, y_pred))

"""svm"""

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, classification_report

# Load the data
data = pd.read_csv('/content/drive/MyDrive/research-1/dataset/heart.csv')

# Convert categorical variables to dummy/indicator variables
data = pd.get_dummies(data, columns=['Sex', 'ChestPainType', 'RestingECG', 'ExerciseAngina', 'ST_Slope'], drop_first=True)

# Separate features and target variable
X = data.drop('HeartDisease', axis=1)
y = data['HeartDisease']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create an SVM classifier
model = SVC(kernel='linear', random_state=42)

# Train the model
model.fit(X_train, y_train)

# Make predictions on the test set
y_pred = model.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy:.2f}")

# Display classification report
print("Classification Report:\n", classification_report(y_test, y_pred))



"""**Random Forest**

---


"""

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import matplotlib.pyplot as plt
import seaborn as sns

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

"""**String to numeric**"""

from sklearn.preprocessing import LabelEncoder
data.head(2)

li = ['Sex','ChestPainType','RestingECG','ExerciseAngina','ST_Slope']
for i in li:
    data[i] = LabelEncoder().fit_transform(data[i])

"""**Correlation of data**"""

sns.heatmap(data.corr(),annot=True,fmt ='.2f',mask = np.triu(data.corr()))

import warnings
warnings.filterwarnings('ignore')

"""Outliers"""

plt.figure(figsize=(15,15))
for i in enumerate(data.columns):
    plt.subplot(3,4,i[0]+1)
    sns.boxplot(data[i[1]])

def remove(data):
  Q1 = np.percentile(data, 25, interpolation = 'midpoint')

# Third quartile (Q3)
  Q3 = np.percentile(data, 75, interpolation = 'midpoint')
  iqr = Q3 - Q1
  percentile25 = data.quantile(0.25)
  percentile75 = data.quantile(0.75)

  upper_limit = percentile75 + 1.5 * iqr
  lower_limit = percentile25 - 1.5 * iqr
  data = np.where(
    data > upper_limit,upper_limit,
    np.where(
        data < lower_limit,
        lower_limit,
        data
    )
  )
  return data

li =['RestingBP','Cholesterol','MaxHR','Oldpeak']
for i in li:
    data[i]=remove(data[i])

plt.figure(figsize=(15,15))
for i in enumerate(data.columns):
    plt.subplot(3,4,i[0]+1)
    sns.boxplot(data[i[1]])

plt.figure(figsize=(15,15))
for i in enumerate(data.columns):
    plt.subplot(3,4,i[0]+1)
    sns.distplot(data[i[1]])

feature = data.drop('HeartDisease',axis=1)
label = data['HeartDisease']

from sklearn.model_selection import train_test_split
trainF,testF,trainL,testL = train_test_split(feature,label,random_state=42,test_size=.2)

from sklearn.ensemble import RandomForestClassifier
model = RandomForestClassifier()
model.fit(trainF,trainL)
pred = model.predict(testF)

from sklearn.metrics import classification_report
print(classification_report(pred,testL))

label.value_counts()

from sklearn.preprocessing import StandardScaler

li = ['Age','RestingBP','Cholesterol','MaxHR','Oldpeak']
for i in li:
    feature[i] = StandardScaler().fit_transform(feature[[i]])

plt.figure(figsize=(15,15))
for i in enumerate(data.columns):
    plt.subplot(3,4,i[0]+1)
    sns.distplot(data[i[1]])

for i in [2,3,4,5]:
    x = data['Oldpeak']**(1/i)
    sns.distplot(x)
    plt.show()

data['Oldpeak'] = data['Oldpeak']**(1/3)



data['HeartDisease'].value_counts()

data.shape

from imblearn.combine  import SMOTEENN
os = SMOTEENN(random_state = 42)
feature,label = os.fit_resample(feature,label)

from sklearn.model_selection import train_test_split
trainF,testF,trainL,testL = train_test_split(feature,label,random_state=42,test_size=.2)

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, f1_score, accuracy_score, recall_score, roc_auc_score
from sklearn.model_selection import train_test_split

# Splitting the data into training and testing sets
trainF, testF, trainL, testL = train_test_split(feature, label, random_state=42, test_size=.2)

# Initializing the Random Forest classifier
RF_model = RandomForestClassifier(class_weight="balanced", random_state=101)

# Training the model
RF_model.fit(trainF, trainL)

# Making predictions
y_pred = RF_model.predict(testF)

# Evaluating the model
print(classification_report(y_pred, testL))

# Calculating evaluation metrics
rf_f1 = f1_score(testL, y_pred)
rf_acc = accuracy_score(testL, y_pred)
rf_recall = recall_score(testL, y_pred)
rf_auc = roc_auc_score(testL, y_pred)

from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, f1_score, accuracy_score, recall_score, roc_auc_score

# Assuming 'feature' and 'label' are already defined
# Splitting the data into training and testing sets
trainF, testF, trainL, testL = train_test_split(feature, label, random_state=42, test_size=.2)

# Initializing the Decision Tree classifier
decision_tree_model = DecisionTreeClassifier()

# Training the model
decision_tree_model.fit(trainF, trainL)

# Making predictions
pred = decision_tree_model.predict(testF)

# Evaluating the model
print(classification_report(pred, testL))

# Calculating evaluation metrics
dt_f1 = f1_score(testL, pred)
dt_acc = accuracy_score(testL, pred)
dt_recall = recall_score(testL, pred)
dt_auc = roc_auc_score(testL, pred)

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, f1_score, accuracy_score, recall_score, roc_auc_score

# Assuming 'feature' and 'label' are already defined
# Splitting the data into training and testing sets
trainF, testF, trainL, testL = train_test_split(feature, label, random_state=42, test_size=.2)

# Initializing the logistic regression model
logistic_model = LogisticRegression()

# Training the model
logistic_model.fit(trainF, trainL)

# Making predictions
pred = logistic_model.predict(testF)

# Evaluating the model
print(classification_report(pred, testL))

# Calculating evaluation metrics
log_f1 = f1_score(testL, pred)
log_acc = accuracy_score(testL, pred)
log_recall = recall_score(testL, pred)
log_auc = roc_auc_score(testL, pred)

from sklearn.ensemble import GradientBoostingClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, f1_score, accuracy_score, recall_score, roc_auc_score

# Assuming 'feature' and 'label' are already defined
# Splitting the data into training and testing sets
trainF, testF, trainL, testL = train_test_split(feature, label, random_state=42, test_size=.2)

# Initializing the Gradient Boosting classifier
gradient_boosting_model = GradientBoostingClassifier()

# Training the model
gradient_boosting_model.fit(trainF, trainL)

# Making predictions
pred = gradient_boosting_model.predict(testF)

# Evaluating the model
print(classification_report(pred, testL))

# Calculating evaluation metrics
gb_f1 = f1_score(testL, pred)
gb_acc = accuracy_score(testL, pred)
gb_recall = recall_score(testL, pred)
gb_auc = roc_auc_score(testL, pred)

from sklearn.svm import SVC
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, f1_score, accuracy_score, recall_score, roc_auc_score

# Assuming 'feature' and 'label' are already defined
# Splitting the data into training and testing sets
trainF, testF, trainL, testL = train_test_split(feature, label, random_state=42, test_size=.2)

# Initializing the SVM classifier
svm_model = SVC()

# Training the model
svm_model.fit(trainF, trainL)

# Making predictions
pred = svm_model.predict(testF)

# Evaluating the model
print(classification_report(pred, testL))

# Calculating evaluation metrics
svm_f1 = f1_score(testL, pred)
svm_acc = accuracy_score(testL, pred)
svm_recall = recall_score(testL, pred)
svm_auc = roc_auc_score(testL, pred)

import xgboost as xgb
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, f1_score, accuracy_score, recall_score, roc_auc_score

# Assuming 'feature' and 'label' are already defined
# Splitting the data into training and testing sets
trainF, testF, trainL, testL = train_test_split(feature, label, random_state=42, test_size=.2)

# Convert data into DMatrix format
dtrain = xgb.DMatrix(trainF, label=trainL)
dtest = xgb.DMatrix(testF)

# Parameters for XGBoost
params = {
    'objective': 'binary:logistic',
    'eval_metric': 'logloss'
}

# Training the XGBoost model
num_round = 100
xgb_model = xgb.train(params, dtrain, num_round)

# Making predictions
pred_probs = xgb_model.predict(dtest)
pred = [1 if x > 0.5 else 0 for x in pred_probs]

# Evaluating the model
print(classification_report(testL, pred))

# Calculating evaluation metrics
xgb_grid_f1 = f1_score(testL, pred)
xgb_grid_acc = accuracy_score(testL, pred)
xgb_grid_recall = recall_score(testL, pred)
xgb_grid_auc = roc_auc_score(testL, pred)

from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, f1_score, accuracy_score, recall_score, roc_auc_score

# Assuming 'feature' and 'label' are already defined
# Splitting the data into training and testing sets
trainF, testF, trainL, testL = train_test_split(feature, label, random_state=42, test_size=.2)

# Initializing the KNN classifier
knn_model = KNeighborsClassifier()

# Training the model
knn_model.fit(trainF, trainL)

# Making predictions
pred = knn_model.predict(testF)

# Evaluating the model
print(classification_report(pred, testL))

# Calculating evaluation metrics
knn_f1 = f1_score(testL, pred)
knn_acc = accuracy_score(testL, pred)
knn_recall = recall_score(testL, pred)
knn_auc = roc_auc_score(testL, pred)

import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# Create DataFrame with model names and their corresponding metrics
compare = pd.DataFrame({
    "Model": ["Logistic Regression", "SVM", "KNN", "Decision Tree", "Random Forest","GradientBoost", "XGBoost"],
    "F1": [log_f1,svm_f1,knn_f1, dt_f1, rf_f1, gb_f1, xgb_grid_f1],
    "Recall": [log_recall, svm_recall, knn_recall, dt_recall, rf_recall, gb_recall, xgb_grid_recall],
    "Accuracy": [log_acc, svm_acc, knn_acc, dt_acc, rf_acc, gb_acc, xgb_grid_acc],
    "ROC_AUC": [log_auc, svm_auc, knn_auc, dt_auc, rf_auc, gb_auc,xgb_grid_auc]
})

# Define a function to label the bars in the plot
def labels(ax):
    for p in ax.patches:
        width = p.get_width()                        # get bar length
        ax.text(width,                               # set the text at 1 unit right of the bar
                p.get_y() + p.get_height() / 2,      # get Y coordinate + X coordinate / 2
                '{:1.3f}'.format(width),             # set variable to display, 2 decimals
                ha='left',                           # horizontal alignment
                va='center')                         # vertical alignment

# Create subplots for each metric comparison
plt.figure(figsize=(14, 14))

plt.subplot(411)
compare = compare.sort_values(by="F1", ascending=False)
ax = sns.barplot(x="F1", y="Model", data=compare, palette="Blues_d")
labels(ax)
plt.xlabel('F1 Score')
plt.ylabel('Model')
plt.title('Comparison of Models based on F1 Score')

plt.subplot(412)
compare = compare.sort_values(by="Recall", ascending=False)
ax = sns.barplot(x="Recall", y="Model", data=compare, palette="Blues_d")
labels(ax)
plt.xlabel('Recall')
plt.ylabel('Model')
plt.title('Comparison of Models based on Recall')

plt.subplot(413)
compare = compare.sort_values(by="Accuracy", ascending=False)
ax = sns.barplot(x="Accuracy", y="Model", data=compare, palette="Blues_d")
labels(ax)
plt.xlabel('Accuracy')
plt.ylabel('Model')
plt.title('Comparison of Models based on Accuracy')

plt.subplot(414)
compare = compare.sort_values(by="ROC_AUC", ascending=False)
ax = sns.barplot(x="ROC_AUC", y="Model", data=compare, palette="Blues_d")
labels(ax)
plt.xlabel('ROC-AUC')
plt.ylabel('Model')
plt.title('Comparison of Models based on ROC-AUC')

plt.tight_layout()
plt.show()